import gymnasium as gym
from gymnasium import Wrapper

from zerothoptim import train_0th_optim
from population import train_population
from utils import run_episode, AdaptativeStdReduction

import torch
import torch.multiprocessing as mp

from stable_baselines3 import DDPG
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise

import numpy as np

import logging

from tqdm import tqdm


class TorchWrapper(Wrapper):
    """
        Wrapper for our gym env to convert numpy arrays into 
        torch tensors. Simplifies passing param between 
        our RL model and our gym env.

        Generated by Claude Sonnet 4.
    """

    def __init__(self, env, device='cpu'):
        super().__init__(env)

        self.device = device

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        obs = torch.from_numpy(obs).float().to(self.device)
        return obs, info

    def step(self, action):
        # Convert action back to numpy if needed
        if isinstance(action, torch.Tensor):
            action = action.cpu().numpy()

        obs, reward, terminated, truncated, info = self.env.step(action)
        obs = torch.from_numpy(obs).float().to(self.device)
        reward = torch.tensor(reward, dtype=torch.float32, device=self.device)

        return obs, reward, terminated, truncated, info


def training_worker(worker_id, train_env, hyperparams):
    """
    Worker function that runs the training loop

    Args:
        worker_id: Identifier for the worker (0 or 1)
        train_env: Training environment
        hyperparams: Dictionary containing hyperparameters for this worker
    """
    print(
        f"Worker {worker_id} starting training with hyperparams: {hyperparams}")

    for i in range(2):
        policy = train_0th_optim(
            train_env,
            500,
            number_evaluation=5,
            adaptive_std=AdaptativeStdReduction(
                std=hyperparams['std'],
                decay_rate=hyperparams['decay_rate'],
                window_size=hyperparams['window_size'],
                reward_threshold=hyperparams['reward_threshold']
            ),
            lr=hyperparams['lr'],
            hidden_dims=hyperparams['hidden_dims'],
            log_file_name=hyperparams['log_file_name']
        )

    print(f"Worker {worker_id} completed training")


def ddpg_policy(env):
    """
        Returns trained ddpg model
    """
    # The noise objects for DDPG
    n_actions = env.action_space.shape[-1]
    action_noise = NormalActionNoise(mean=np.zeros(
        n_actions), sigma=0.1 * np.ones(n_actions))

    model = DDPG("MlpPolicy", env, action_noise=action_noise, verbose=1)
    model.learn(total_timesteps=5000, log_interval=100)
    return model


def zeroth_policy(env):
    policy = train_0th_optim(
        env,
        500,
        number_evaluation=5,
        adaptive_std=AdaptativeStdReduction(
            std=0.5,
            decay_rate=0.95,
            window_size=3,
            reward_threshold=100
        ),
        lr=0.01,
        hidden_dims=128,
        log_file_name="dump.txt"
    )
    return policy


def population_policy(env):
    policy = train_population(
        env,
        500,
        number_evaluation=5,
        n=10,
        adaptive_std=AdaptativeStdReduction(
            std=0.5,
            decay_rate=0.95,
            window_size=3,
            reward_threshold=100
        ),
        log_file_name="population_10_adapt_128.txt"
    )
    return policy


def main():
    train_env = TorchWrapper(
        gym.make("LunarLander-v3", continuous=True, render_mode=None))
    ddpg = ddpg_policy(train_env)
    zeroth = zeroth_policy(train_env)
    pop = population_policy(train_env)

    display_env = TorchWrapper(gym.make("LunarLander-v3", continuous=True, render_mode=None))
    display_env = TorchWrapper(
        gym.make("LunarLander-v3", continuous=True, render_mode=None))

    file_handler = logging.FileHandler("logs/baseline_3.txt")
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter('%(message)s'))
    logging.getLogger().addHandler(file_handler)

    for i in tqdm(range(100), desc="Eval Episodes", unit="episode"):
        ddpg_r = run_episode(display_env, lambda obs: ddpg.predict(obs)[0])
        random_r = run_episode(display_env, lambda obs: display_env.action_space.sample())
        zero_r = run_episode(display_env, lambda obs: zeroth(obs))
        pop_r = run_episode(display_env, lambda obs: pop(obs))
        log_str = f"{ddpg_r}, {random_r}, {zero_r}, {pop_r}"
        logging.info(log_str)
        tqdm.write(log_str)

    # mp.set_start_method('spawn', force=True)
    #
    # hyperparams_set1 = {
    #     'std': 0.5,
    #     'decay_rate': 0.95,
    #     'window_size': 3,
    #     'reward_threshold': 100,
    #     'lr': 0.01,
    #     'hidden_dims': 128,
    #     'log_file_name': "adaptative.txt"
    # }
    #
    # # Create processes
    # processes = []
    # p1 = mp.Process(target=training_worker, args=(1, train_env, hyperparams_set1))
    # processes.append(p1)
    # for p in processes:
    #     p.start()
    # for p in processes:
    #     p.join()
    #
    # print("All workers completed!")
    # policy = train_population(train_env, 1000, runs_per_episode=5, std=1, n = 50)
    train_env.close()

    # display_env = TorchWrapper(gym.make("LunarLander-v3", continuous=True, render_mode="human"))
    # r = run_episode(display_env, lambda obs: policy(obs))
    # print(f"Final Reward: {r}")
    # display_env.close()


if __name__ == "__main__":
    main()
