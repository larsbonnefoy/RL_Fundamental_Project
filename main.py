import gymnasium as gym
from gymnasium import Wrapper
from policy import ParametricPolicy
from zerothoptim import produce_perturbations
import torch

class TorchWrapper(Wrapper):
    """
        Wrapper for our gym env to convert numpy arrays into 
        torch tensors. Simplifies passing param between 
        our RL model and our gym env.

        Generated by Claude Sonnet 4.
    """
    def __init__(self, env, device='cpu'):
        super().__init__(env)
        self.device = device
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        obs = torch.from_numpy(obs).float().to(self.device)
        return obs, info
    
    def step(self, action):
        # Convert action back to numpy if needed
        if isinstance(action, torch.Tensor):
            action = action.cpu().numpy()

        obs, reward, terminated, truncated, info = self.env.step(action)
        obs = torch.from_numpy(obs).float().to(self.device)
        reward = torch.tensor(reward, dtype=torch.float32, device=self.device)

        return obs, reward, terminated, truncated, info


def train_epsiode(env, action_function):
    """
        Produces one epsiode from our environment. Selects an action from our 
        action function which should be a function which takes as argument and observation.

        :param env: our gym environment
        :param action_function: function using observations to produce a new action
    """
    # initial state
    observation, info = env.reset()
    episode_over = False
    total_reward = 0

    while not episode_over:
        # action = env.action_space.sample()
        action = action_function(observation)
        print(action)

        # Take the action and see what happens
        observation, reward, terminated, truncated, info = env.step(action)

        total_reward += reward
        episode_over = terminated or truncated

    print(f"Episode finished! Total reward: {total_reward}")
    return reward


def main():
    env = TorchWrapper(gym.make("LunarLander-v3", continuous=True, render_mode="human"))

    policy: ParametricPolicy = ParametricPolicy()
    p_p, n_p = produce_perturbations(policy.get_parameters())

    # Samples provides a numpy array
    # state = env.observation_space.sample()
    # state = torch.FloatTensor(state)

    with torch.no_grad():
        train_epsiode(env, lambda obs: policy(obs))




    # print(f"Starting observation: {observation}")
    #
    env.close()


if __name__ == "__main__":
    main()
