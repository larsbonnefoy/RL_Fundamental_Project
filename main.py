import gymnasium as gym
from gymnasium import Wrapper
from zerothoptim import train_0th_optim
from population import train_population
from utils import run_episode
import torch

class TorchWrapper(Wrapper):
    """
        Wrapper for our gym env to convert numpy arrays into 
        torch tensors. Simplifies passing param between 
        our RL model and our gym env.

        Generated by Claude Sonnet 4.
    """
    def __init__(self, env, device='cpu'):
        super().__init__(env)
        self.device = device
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        obs = torch.from_numpy(obs).float().to(self.device)
        return obs, info
    
    def step(self, action):
        # Convert action back to numpy if needed
        if isinstance(action, torch.Tensor):
            action = action.cpu().numpy()

        obs, reward, terminated, truncated, info = self.env.step(action)
        obs = torch.from_numpy(obs).float().to(self.device)
        reward = torch.tensor(reward, dtype=torch.float32, device=self.device)

        return obs, reward, terminated, truncated, info



def main():
    train_env = TorchWrapper(gym.make("LunarLander-v3", continuous=True, render_mode=None))
    #policy = train_0th_optim(train_env, 1000, runs_per_episode=4, std=0.1, lr=0.01)
    policy = train_population(train_env, 1000, runs_per_episode=5, std=1)
    train_env.close()

    display_env = TorchWrapper(gym.make("LunarLander-v3", continuous=True, render_mode="human")) 
    r = run_episode(display_env, lambda obs: policy(obs))
    print(f"Final Reward: {r}")
    display_env.close()


if __name__ == "__main__":
    main()
