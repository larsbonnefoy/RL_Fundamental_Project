import gymnasium as gym
from gymnasium import Wrapper
from zerothoptim import train_0th_optim
from population import train_population
from utils import run_episode, AdaptativeStdReduction
import torch
import torch.multiprocessing as mp

class TorchWrapper(Wrapper):
    """
        Wrapper for our gym env to convert numpy arrays into 
        torch tensors. Simplifies passing param between 
        our RL model and our gym env.

        Generated by Claude Sonnet 4.
    """
    def __init__(self, env, device='cpu'):
        super().__init__(env)
        self.device = device
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        obs = torch.from_numpy(obs).float().to(self.device)
        return obs, info
    
    def step(self, action):
        # Convert action back to numpy if needed
        if isinstance(action, torch.Tensor):
            action = action.cpu().numpy()

        obs, reward, terminated, truncated, info = self.env.step(action)
        obs = torch.from_numpy(obs).float().to(self.device)
        reward = torch.tensor(reward, dtype=torch.float32, device=self.device)

        return obs, reward, terminated, truncated, info

def training_worker(worker_id, train_env, hyperparams):
    """
    Worker function that runs the training loop
    
    Args:
        worker_id: Identifier for the worker (0 or 1)
        train_env: Training environment
        hyperparams: Dictionary containing hyperparameters for this worker
    """
    print(f"Worker {worker_id} starting training with hyperparams: {hyperparams}")
    
    for i in range(2):
        policy = train_0th_optim(
            train_env, 
            500, 
            number_evaluation=5, 
            adaptive_std=AdaptativeStdReduction(
                std=hyperparams['std'],
                decay_rate=hyperparams['decay_rate'], 
                window_size=hyperparams['window_size'],
                reward_threshold=hyperparams['reward_threshold']
            ), 
            lr=hyperparams['lr'], 
            hidden_dims=hyperparams['hidden_dims'], 
            log_file_name=hyperparams['log_file_name']
        )
    
    print(f"Worker {worker_id} completed training")

def main():
    train_env = TorchWrapper(gym.make("LunarLander-v3", continuous=True, render_mode=None))

    mp.set_start_method('spawn', force=True)
    
    hyperparams_set1 = {
        'std': 0.5,
        'decay_rate': 0.95,
        'window_size': 3,
        'reward_threshold': 100,
        'lr': 0.01,
        'hidden_dims': 128,
        'log_file_name': "adaptative.txt"
    }
    
    hyperparams_set2 = {
        'std': 0.5,
        'decay_rate': 1,
        'window_size': 3,
        'reward_threshold': 100,
        'lr': 0.01,
        'hidden_dims': 128,
        'log_file_name': "constant.txt"
    }

        # Create processes
    processes = []
    
    p1 = mp.Process(target=training_worker, args=(1, train_env, hyperparams_set1))
    processes.append(p1)
    
    p2 = mp.Process(target=training_worker, args=(2, train_env, hyperparams_set2))
    processes.append(p2)
    
    for p in processes:
        p.start()
    for p in processes:
        p.join()
    
    print("All workers completed!")
    #policy = train_population(train_env, 1000, runs_per_episode=5, std=1, n = 50)
    train_env.close()

    #display_env = TorchWrapper(gym.make("LunarLander-v3", continuous=True, render_mode="human")) 
    #r = run_episode(display_env, lambda obs: policy(obs))
    #print(f"Final Reward: {r}")
    #display_env.close()


if __name__ == "__main__":
    main()
